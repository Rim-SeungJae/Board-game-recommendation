# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S6BLLFDCo-Eau7qTfQdG-xKAxhCrajFE
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset
import csv

# 데이터 로드 및 전처리
bg_rating = pd.read_csv('/content/bgg-15m-reviews.csv', on_bad_lines='skip',low_memory=False)
bg_rating.drop('comment', axis=1, inplace=True)
bg_rating = bg_rating[['user', 'name', 'rating']].dropna()

# 사용자 ID와 아이템 ID를 인덱스로 변환
user_ids = bg_rating['user'].astype('category').cat.codes
item_ids = bg_rating['name'].astype('category').cat.codes
bg_rating['rating'] = pd.to_numeric(bg_rating['rating'], errors='coerce')
ratings = bg_rating['rating'].astype('float32')

# 훈련 데이터셋과 테스트 데이터셋 분리
train_data, test_data, train_labels, test_labels = train_test_split(
    np.vstack((user_ids, item_ids)).T, ratings, test_size=0.2, random_state=42)

class RatingDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels.values

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        user = self.data[idx, 0]
        item = self.data[idx, 1]
        rating = self.labels[idx]

        # Convert each to a tensor
        user_tensor = torch.tensor(user, dtype=torch.long)
        item_tensor = torch.tensor(item, dtype=torch.long)
        rating_tensor = torch.tensor(rating, dtype=torch.float32)

        input_tensor = torch.stack([user_tensor, item_tensor], dim=-1)

        return input_tensor, rating_tensor

train_dataset = RatingDataset(train_data, train_labels)
test_dataset = RatingDataset(test_data, test_labels)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

import torch.nn as nn
import torch.nn.functional as F

class NCF(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim, hidden_layers):
        super(NCF, self).__init__()
        # GMF part
        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim)
        self.item_embedding_gmf = nn.Embedding(num_items, embedding_dim)

        # MLP part
        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim)
        self.item_embedding_mlp = nn.Embedding(num_items, embedding_dim)

        self.mlp_layers = nn.Sequential()
        input_dim = 2 * embedding_dim

        self.mlp_layers.add_module("linear_0", nn.Linear(input_dim, hidden_layers[0]))
        self.mlp_layers.add_module("relu_0", nn.ReLU())

        for i, hidden_dim in enumerate(hidden_layers[1:], start=1):
            self.mlp_layers.add_module(f"linear_{i}", nn.Linear(hidden_layers[i-1], hidden_dim))
            self.mlp_layers.add_module(f"relu_{i}", nn.ReLU())

        # Combine GMF and MLP
        self.final_linear = nn.Linear(hidden_layers[-1] + embedding_dim, 1)

    def forward(self, x):
        user_id = x[:, 0]
        item_id = x[:, 1]

        # GMF part
        gmf_user_embedding = self.user_embedding_gmf(user_id)
        gmf_item_embedding = self.item_embedding_gmf(item_id)
        gmf_output = gmf_user_embedding * gmf_item_embedding

        # MLP part
        mlp_user_embedding = self.user_embedding_mlp(user_id)
        mlp_item_embedding = self.item_embedding_mlp(item_id)
        mlp_input = torch.cat([mlp_user_embedding, mlp_item_embedding], dim=-1)
        mlp_output = self.mlp_layers(mlp_input)

        # Combine GMF and MLP outputs
        concat_output = torch.cat([gmf_output, mlp_output], dim=-1)
        prediction = torch.sigmoid(self.final_linear(concat_output))

        return prediction.squeeze()

# 모델 초기화
num_users = len(bg_rating['user'].unique())
num_items = len(bg_rating['name'].unique())
embedding_dim = 20
hidden_layers = [64, 32]

model = NCF(num_users, num_items, embedding_dim, hidden_layers)

print(model)

import torch.optim as optim

# 손실 함수 및 옵티마이저 정의
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 학습 루프
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels.squeeze())
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}")

# 테스트 평가
model.eval()
with torch.no_grad():
    mse = 0.0
    for inputs, labels in test_loader:
        outputs = model(inputs)
        mse += criterion(outputs, labels.squeeze()).item()

    mse /= len(test_loader)
    print(f"Test MSE: {mse}")